{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "38740d3277777e2cd7c6c2cc9d8addf5118fdf3f82b1b39231fd12aeac8aee8b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "# dataset = np.loadtxt('data/andmal2020/BenignCSVs/Ben0.csv',dtype=int,skiprows=1,usecols=cols,delimiter=',')\n",
    "\n",
    "# 使用pandas批量读取大文件\n",
    "filelist1 = [\n",
    "    'data/andmal2020/BenignCSVs/Ben0.csv',\n",
    "    'data/andmal2020/BenignCSVs/Ben1.csv',\n",
    "    'data/andmal2020/BenignCSVs/Ben2.csv',\n",
    "    'data/andmal2020/BenignCSVs/Ben3.csv',\n",
    "    'data/andmal2020/BenignCSVs/Ben4.csv'\n",
    "]\n",
    "# 暂时先用10个\n",
    "filelist2 = [\n",
    "    'data/andmal2020/Malicious-CSVs/mal1.csv',\n",
    "    'data/andmal2020/Malicious-CSVs/mal2.csv',\n",
    "    'data/andmal2020/Malicious-CSVs/mal3.csv',\n",
    "    'data/andmal2020/Malicious-CSVs/mal4.csv',\n",
    "    'data/andmal2020/Malicious-CSVs/mal5.csv',\n",
    "    'data/andmal2020/Malicious-CSVs/mal6.csv',\n",
    "    'data/andmal2020/Malicious-CSVs/mal7.csv',\n",
    "    'data/andmal2020/Malicious-CSVs/mal8.csv',\n",
    "    'data/andmal2020/Malicious-CSVs/mal9.csv',\n",
    "    'data/andmal2020/Malicious-CSVs/mal10.csv',\n",
    "    # 'data/andmal2020/Malicious-CSVs/mal11.csv',\n",
    "    # 'data/andmal2020/Malicious-CSVs/mal12.csv',\n",
    "    # 'data/andmal2020/Malicious-CSVs/mal13.csv',\n",
    "    # 'data/andmal2020/Malicious-CSVs/mal14.csv',\n",
    "]\n",
    "chunksize = 5000\n",
    "cols = list(range(2,4))\n",
    "# dataset = pd.read_csv('data/andmal2020/BenignCSVs/Ben0.csv',dtype=int,skiprows=1,iterator=True,chunksize=chunk_size,usecols=cols)\n",
    "\n",
    "\n",
    "def read_chunk(filelist,cols,chunksize):\n",
    "    for file in filelist:\n",
    "        dataset = pd.read_csv(file,dtype=int,skiprows=1,iterator=True,chunksize=chunksize,usecols=cols)\n",
    "        for chunk in dataset:\n",
    "            yield chunk.values[0]\n",
    "\n",
    "ben_iter = read_chunk(filelist1,cols,chunksize)\n",
    "def get_ben(ben_iter):\n",
    "    return next(ben_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "mal_iters = [pd.read_csv(file,dtype=int,skiprows=1,iterator=True,chunksize=chunksize/10,usecols=cols) for file in filelist2]\n",
    "\n",
    "# print(len(mal_iters))\n",
    "\n",
    "def get_mal(mal_iters):\n",
    "    result = np.array([])\n",
    "    for mal_iter in mal_iters:\n",
    "        # print(next(mal_iter).values[0])\n",
    "        result = np.append(result,next(mal_iter).values[0])\n",
    "    return result\n",
    "\n",
    "get_mal(mal_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2,)\n",
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "print(get_ben(ben_iter).shape)\n",
    "print(get_mal(mal_iters).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([  1,   0, 784,   0,   0,   0,   1,   1,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   1,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "      dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "data = dataset.get_chunk()\n",
    "data.shape\n",
    "data.values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "benign_file_list=['data/andmal2020/BenignCSVs/Ben0.csv']\n",
    "\n",
    "def read_benign():\n",
    "    pass \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malware_file_list = ['']\n",
    "def read_malware():\n",
    "    pass\n",
    "dataset = pd.read_csv('data/andmal2020/BenignCSVs/Ben0.csv')\n",
    "pd.read_csv(,)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd \n",
    "# 对数据进行处理和分割\n",
    "X,y = dataset[:,:-1],dataset[:,-1].astype(int)\n",
    "# print(X.shape,y.shape)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
    "print(pd.value_counts(y_train))\n",
    "print(pd.value_counts(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1使用KNN进行分类\n",
    "from sklearn import neighbors\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn_classifier = neighbors.KNeighborsClassifier(n_neighbors=5,weights='uniform')\n",
    "\n",
    "knn_classifier.fit(X_train,y_train)\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "knn_score = accuracy_score(y_test,y_pred,normalize=True)\n",
    "print(knn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2使用决策树进行分类\n",
    "from sklearn import tree \n",
    "from sklearn.metrics import accuracy_score\n",
    "dt_classifier = tree.DecisionTreeClassifier(max_depth=15,criterion='entropy')\n",
    "dt_classifier.fit(X=X_train ,y=y_train)\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "dt_score = accuracy_score(y_test,y_pred)\n",
    "print(dt_score,dt_classifier.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3使用bayes进行分类\n",
    "from sklearn import naive_bayes\n",
    "# nb_classifier = naive_bayes.GaussianNB()\n",
    "# nb_classifier = naive_bayes.MultinomialNB()\n",
    "nb_classifier = naive_bayes.BernoulliNB()\n",
    "nb_classifier.fit(X_train,y_train)\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "nb_score = accuracy_score(y_test,y_pred)\n",
    "print(nb_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4使用逻辑回归.本身是不能多分类的。所以准确率很低。\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_classifier = LogisticRegression()\n",
    "lr_classifier.fit(X_train,y_train)\n",
    "y_pred = lr_classifier.predict(X_test)\n",
    "lr_score = accuracy_score(y_test,y_pred)\n",
    "print(lr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5使用svm-linear\n",
    "# from sklearn import svm\n",
    "# svm_classifier = svm.SVC(kernel='linear')\n",
    "# svm_classifier.fit(X_train,y_train)\n",
    "# y_pred = svm_classifier.predict(X_test)\n",
    "# svm_score = accuracy_score(y_test,y_pred)\n",
    "# print(svm_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用svm-rbf\n",
    "# from sklearn import svm\n",
    "# svm_classifier = svm.SVC(kernel='poly',degree=5)\n",
    "# svm_classifier.fit(X_train,y_train)\n",
    "# y_pred = svm_classifier.predict(X_test)\n",
    "# accuracy_score(y_test,y_pred)\n",
    "# print(svm_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1使用随机森林进行分类\n",
    "from sklearn import ensemble\n",
    "rf_classifier = ensemble.RandomForestClassifier()\n",
    "rf_classifier.fit(X_train,y_train)\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "rf_score = accuracy_score(y_test,y_pred)\n",
    "print(rf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2使用adaboost\n",
    "from sklearn import ensemble\n",
    "from sklearn import tree \n",
    "dt = tree.DecisionTreeClassifier(max_depth=5,criterion='entropy')\n",
    "ab_classifier = ensemble.AdaBoostClassifier(dt)\n",
    "ab_classifier.fit(X_train,y_train)\n",
    "y_pred = ab_classifier.predict(X_test)\n",
    "ab_score = accuracy_score(y_test,y_pred)\n",
    "print(ab_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7使用SGD+SVM\n",
    "from sklearn import linear_model\n",
    "sgd_classifier = linear_model.SGDClassifier(loss='squared_hinge',l1_ratio=0.15,learning_rate='optimal',n_jobs=5,penalty='l2',random_state=True)\n",
    "sgd_classifier.fit(X_train,y_train)\n",
    "y_pred = sgd_classifier.predict(X_test)\n",
    "sgd_score = accuracy_score(y_test,y_pred)\n",
    "print(sgd_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8使用OneVsRestClassifier+logisticsRegression进行多分类\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "ovr_classifier = OneVsRestClassifier(DecisionTreeClassifier(max_depth=10,criterion='entropy'),n_jobs=5)\n",
    "ovr_classifier.fit(X_train,y_train)\n",
    "y_pred = ovr_classifier.predict(X_test)\n",
    "ovr_score = accuracy_score(y_test,y_pred)\n",
    "print(ovr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对数据内容进行统计——信息增益\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mutual_increase = mutual_info_classif(X,y)\n",
    "print(mutual_increase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA降维\n",
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(n_components=0.95)\n",
    "\n",
    "data_pca = pca.fit_transform(X)\n",
    "print(pca.n_components_,pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对降维后的数据使用随机森林看一下。对准确率的影响没有想象中那么大。\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x1,x2,y1,y2 = train_test_split(data_pca,y,test_size=0.3)\n",
    "rf_classifier2 = ensemble.RandomForestClassifier()\n",
    "rf_classifier2.fit(x1,y1)\n",
    "y_pred = rf_classifier2.predict(x2)\n",
    "rf_score2 = accuracy_score(y2,y_pred)\n",
    "print(rf_score2)"
   ]
  }
 ]
}